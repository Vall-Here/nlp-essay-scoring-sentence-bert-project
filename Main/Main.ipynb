{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae6b31",
   "metadata": {},
   "source": [
    "use python 3.9.13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fa5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e71314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb105bb",
   "metadata": {},
   "source": [
    "# Rancangan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59569b9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[ Pilih Soal ]: â¬‡ï¸ Stimulus A / Stimulus B\n",
    "\n",
    "[ Stimulus ]:\n",
    "Pemanasan global terjadi karena...\n",
    "\n",
    "[ Pertanyaan ]:\n",
    "Apa yang menjadi tantangan...\n",
    "\n",
    "[ Jawaban Siswa ]:\n",
    "âœŽ **\\*\\*\\*\\***\\_**\\*\\*\\*\\***--\n",
    "\n",
    "[ðŸ” Nilai Jawaban]\n",
    "\n",
    "ðŸ“Š Hasil:\n",
    "âœ… Jawaban diklasifikasikan sebagai \"Relevan dan Benar\" (Label: 1)\n",
    "\n",
    "ðŸ§  Alasan: Jawaban mengandung kata kunci \"pindah\", \"ekonomi\", \"sosial\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec212f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c0c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report,f1_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fc0bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ea7e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9a847",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8b1cc",
   "metadata": {},
   "source": [
    "# Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_csv('../Dataset/Data_A/data_train_A.csv')\n",
    "dev_a = pd.read_csv('../Dataset/Data_A/data_dev_A.csv')\n",
    "test_a = pd.read_csv('../Dataset/Data_A/data_test_A.csv')\n",
    "train_b = pd.read_csv('../Dataset/Data_B/data_train_B.csv')\n",
    "dev_b = pd.read_csv('../Dataset/Data_B/data_dev_B.csv')\n",
    "test_b = pd.read_csv('../Dataset/Data_B/data_test_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Train A:\")\n",
    "print(train_a.head())\n",
    "print(\"\\nKolom:\", train_a.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7375a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Train B:\")\n",
    "print(train_b.head())\n",
    "print(\"\\nKolom:\", train_b.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_a = [\"Pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi.\",\n",
    "\"Salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut.\",\n",
    "\"kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan.\",\n",
    "\"Film â€œWaterworldâ€, adalah film fiksi ilmiah yang menunjukkan akibat adanya pemanasan global yang sangat besar sehingga menyebabkan bumi menjadi tertutup oleh lautan.\",\n",
    "\"Negara-negara dan daratan yang dulunya kering menjadi tengelamn karena terjadi kenaikan permukaan air laut.\",\n",
    "\"Penduduk yang dulunya bisa berkehidupan bebas menjadi terpaksa mengungsi ke daratan yang lebih tinggi atau tinggal diatas air.\",\n",
    "\"Apa yang akan menjadi tantangan bagi suatu penduduk ketika terjadi situasi daratan tidak dapat ditinggali kembali karena tengelam oleh naiknya air laut.\"]\n",
    "\n",
    "stimulus_b = [\"Sebuah toko baju berkonsep self-service menawarkan promosi dua buah baju bertema tahun baru seharga Rp50.000,00. sebelum baju bertema tahun baru dibagikan kepada pembeli, sebuah layar akan menampilkan tampilan gambar yang menampilkan kondisi kerja di dalam sebuah pabrik konveksi/pembuatan baju. \",\n",
    "\"Kemudian pembeli diberi program pilihan untuk menyelesaikan pembeliannya atau menyumpangkan Rp50.000,00 untuk dijadikan donasi pembagian baju musim dingin di suatu daerah yang membutuhkan.\",\n",
    "\"Delapan dari sepuluh pembeli memilih untuk memberikan donasi.\",\n",
    "\"Menurut anda mengapa banyak dari pembeli yang memilih berdonasi?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_a_text = \" \".join(stimulus_a)\n",
    "stimulus_b_text = \" \".join(stimulus_b)\n",
    "\n",
    "for df in [train_a, dev_a, test_a]:\n",
    "    df[\"TEXT\"] = stimulus_a_text + \" [SEP] \" + df[\"RESPONSE\"]\n",
    "\n",
    "for df in [train_b, dev_b, test_b]:\n",
    "    df[\"TEXT\"] = stimulus_b_text + \" [SEP] \" + df[\"RESPONSE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_a.info())\n",
    "print(train_a.describe(include='all'))  \n",
    "print(train_b.info())\n",
    "print(train_b.describe(include='all'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='LABEL', data=train_a)\n",
    "plt.title('Distribusi Label (Data_A Train)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='LABEL', data=train_b)\n",
    "plt.title('Distribusi Label (Data_B Train)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e841",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f652ebe",
   "metadata": {},
   "source": [
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ukara = {'yang', 'lebih', 'untuk', 'akan', 'mereka', 'dan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_ukara]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64622e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a[\"clean_response\"] = train_a[\"RESPONSE\"].apply(preprocess)\n",
    "train_b[\"clean_response\"] = train_b[\"RESPONSE\"].apply(preprocess)\n",
    "test_a[\"clean_response\"] = test_a[\"RESPONSE\"].apply(preprocess)\n",
    "test_b[\"clean_response\"] = test_b[\"RESPONSE\"].apply(preprocess)\n",
    "dev_a[\"clean_response\"] = dev_a[\"RESPONSE\"].apply(preprocess)\n",
    "dev_b[\"clean_response\"] = dev_b[\"RESPONSE\"].apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0384f27f",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a7c237",
   "metadata": {},
   "source": [
    "## Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b19aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    sbert_model\n",
    "except NameError:\n",
    "    sbert_model = SentenceTransformer('paraphrase-xlm-r-multilingual-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c5f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     sbert_model\n",
    "# except NameError:\n",
    "#     sbert_model = SentenceTransformer('distiluse-base-multilingual-cased-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeb4050",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc560851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(df, sbert_model):\n",
    "    embeddings = sbert_model.encode(df['clean_response'].tolist())\n",
    "    labels = df['LABEL'].values\n",
    "    return embeddings, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a5169c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_a, train_labels_a = extract_features(train_a, sbert_model)\n",
    "dev_embeddings_a, dev_labels_a = extract_features(dev_a, sbert_model)\n",
    "test_embeddings_a, test_labels_a = extract_features(test_a, sbert_model)\n",
    "\n",
    "train_embeddings_b, train_labels_b = extract_features(train_b, sbert_model)\n",
    "dev_embeddings_b, dev_labels_b = extract_features(dev_b, sbert_model)\n",
    "test_embeddings_b, test_labels_b = extract_features(test_b, sbert_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631153ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c1b62",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ad151",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AESModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AESModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(64, 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372001c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_params(params, train_embeddings, train_labels):\n",
    "    if params['use_smote']:\n",
    "        smote = SMOTE()\n",
    "        X_train, y_train = smote.fit_resample(train_embeddings, train_labels)\n",
    "    else:\n",
    "        X_train, y_train = train_embeddings, train_labels\n",
    "    \n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    \n",
    "    model = AESModel(input_dim=X_train.shape[1])\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), weight_decay=1e-4)\n",
    "    \n",
    "    current_batch_size = params['batch_size']\n",
    "    for epoch in range(params['epochs']):\n",
    "        if params['batch_increase'] > 0 and (epoch+1) % params['increase_freq'] == 0:\n",
    "            current_batch_size = min(current_batch_size + params['batch_increase'], len(train_dataset))\n",
    "            \n",
    "        train_loader = DataLoader(train_dataset, batch_size=current_batch_size, shuffle=True)\n",
    "        \n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_embeddings, train_labels):\n",
    "    params = {\n",
    "        'epochs': trial.suggest_int('epochs', 10, 55),\n",
    "        'batch_size': trial.suggest_int('batch_size', 2, 8),\n",
    "        'batch_increase': trial.suggest_int('batch_increase', 0, 4),\n",
    "        'increase_freq': trial.suggest_int('increase_freq', 2, 4),\n",
    "        'use_smote': trial.suggest_categorical('use_smote', [True, False])\n",
    "    }\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(train_embeddings):\n",
    "        X_train, X_val = train_embeddings[train_idx], train_embeddings[val_idx]\n",
    "        y_train, y_val = train_labels[train_idx], train_labels[val_idx]\n",
    "        \n",
    "        model = train_model_with_params(params, X_train, y_train)\n",
    "        \n",
    "        val_dataset = TensorDataset(torch.FloatTensor(X_val), torch.LongTensor(y_val))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        scores.append(correct / total)\n",
    "    \n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, embeddings, labels):\n",
    "    dataset = TensorDataset(torch.FloatTensor(embeddings), torch.LongTensor(labels))\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_preds.extend(preds.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    f1 = f1_score(all_labels, all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds)\n",
    "    recall = recall_score(all_labels, all_preds)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Incorrect', 'Correct']))\n",
    "    \n",
    "    return {\n",
    "        'f1': f1,\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e14c0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15353f",
   "metadata": {},
   "source": [
    "## Data A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591eb1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, train_embeddings_a, train_labels_a), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29fa5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac901200",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = train_model_with_params(best_params, train_embeddings_a, train_labels_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a63f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation on Validation Set:\")\n",
    "val_metrics = evaluate_model(final_model, dev_embeddings_a, dev_labels_a)\n",
    "\n",
    "print(\"\\nEvaluation on Test Set:\")\n",
    "test_metrics = evaluate_model(final_model, test_embeddings_a, test_labels_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af2851",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Metrics:\")\n",
    "print(f\"Validation - Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "print(f\"Test - Accuracy: {test_metrics['accuracy']:.4f}, F1: {test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61dd8a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff26c62",
   "metadata": {},
   "source": [
    "## Data B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832db5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, train_embeddings_b, train_labels_b), n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e5903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ef1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = train_model_with_params(best_params, train_embeddings_b, train_labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2cfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation on Validation Set:\")\n",
    "val_metrics = evaluate_model(final_model, dev_embeddings_b, dev_labels_b)\n",
    "\n",
    "print(\"\\nEvaluation on Test Set:\")\n",
    "test_metrics = evaluate_model(final_model, test_embeddings_b, test_labels_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41eaba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Metrics:\")\n",
    "print(f\"Validation - Accuracy: {val_metrics['accuracy']:.4f}, F1: {val_metrics['f1']:.4f}\")\n",
    "print(f\"Test - Accuracy: {test_metrics['accuracy']:.4f}, F1: {test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6250ad6",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(model, embeddings, labels, texts):\n",
    "    dataset = TensorDataset(torch.FloatTensor(embeddings), torch.LongTensor(labels))\n",
    "    loader = DataLoader(dataset, batch_size=32)\n",
    "    \n",
    "    wrong_samples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(loader):\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            for j in range(len(labels)):\n",
    "                if preds[j] != labels[j]:\n",
    "                    wrong_samples.append({\n",
    "                        'text': texts[i*32 + j],\n",
    "                        'true': labels[j].item(),\n",
    "                        'predicted': preds[j].item()\n",
    "                    })\n",
    "    \n",
    "    print(f\"Total wrong predictions: {len(wrong_samples)}\")\n",
    "    for sample in wrong_samples[:10]:  # Cetak 10 contoh pertama\n",
    "        print(f\"Text: {sample['text']}\")\n",
    "        print(f\"True: {sample['true']}, Predicted: {sample['predicted']}\\n\")\n",
    "    \n",
    "    return wrong_samples\n",
    "\n",
    "wrong_samples = analyze_errors(final_model, test_embeddings_b, test_labels_b, test_b['RESPONSE'].tolist())\n",
    "wrong_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29993b6e",
   "metadata": {},
   "source": [
    "# Opsi\n",
    "\n",
    "1. Bert prepocessing + Model\n",
    "2. Bert Prepocessing, Model Neural\n",
    "3. Bert ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b353352",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
