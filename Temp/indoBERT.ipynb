{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9ae6b31",
   "metadata": {},
   "source": [
    "use python 3.9.13\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541fa5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e71314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb105bb",
   "metadata": {},
   "source": [
    "# Rancangan\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59569b9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[ Pilih Soal ]: ‚¨áÔ∏è Stimulus A / Stimulus B\n",
    "\n",
    "[ Stimulus ]:\n",
    "Pemanasan global terjadi karena...\n",
    "\n",
    "[ Pertanyaan ]:\n",
    "Apa yang menjadi tantangan...\n",
    "\n",
    "[ Jawaban Siswa ]:\n",
    "‚úé **\\*\\*\\*\\***\\_**\\*\\*\\*\\***--\n",
    "\n",
    "[üîç Nilai Jawaban]\n",
    "\n",
    "üìä Hasil:\n",
    "‚úÖ Jawaban diklasifikasikan sebagai \"Relevan dan Benar\" (Label: 1)\n",
    "\n",
    "üß† Alasan: Jawaban mengandung kata kunci \"pindah\", \"ekonomi\", \"sosial\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52ef724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW  # Perubahan di sini\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0214a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d9a847",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8b1cc",
   "metadata": {},
   "source": [
    "# Data Understanding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd1ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a = pd.read_csv('../Dataset/Data_A/data_train_A.csv')\n",
    "dev_a = pd.read_csv('../Dataset/Data_A/data_dev_A.csv')\n",
    "test_a = pd.read_csv('../Dataset/Data_A/data_test_A.csv')\n",
    "train_b = pd.read_csv('../Dataset/Data_B/data_train_B.csv')\n",
    "dev_b = pd.read_csv('../Dataset/Data_B/data_dev_B.csv')\n",
    "test_b = pd.read_csv('../Dataset/Data_B/data_test_B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa2d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Train A:\")\n",
    "print(train_a.head())\n",
    "print(\"\\nKolom:\", train_a.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f94774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7375a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Train B:\")\n",
    "print(train_b.head())\n",
    "print(\"\\nKolom:\", train_b.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4073ac93",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_a = [\"Pemanasan global terjadi karena peningkatan produksi karbon dioksida yang dihasilkan oleh pembakaran fosil dan konsumsi bahan bakar yang tinggi.\",\n",
    "\"Salah satu akibat adalah mencairnya es abadi di kutub utara dan selatan yang menimbulkan naiknya ketinggian air laut.\",\n",
    "\"kenaikan air laut akan terjadi terus menerus meskipun dalam hitungan centimeter akan mengakibatkan perubahan yang signifikan.\",\n",
    "\"Film ‚ÄúWaterworld‚Äù, adalah film fiksi ilmiah yang menunjukkan akibat adanya pemanasan global yang sangat besar sehingga menyebabkan bumi menjadi tertutup oleh lautan.\",\n",
    "\"Negara-negara dan daratan yang dulunya kering menjadi tengelamn karena terjadi kenaikan permukaan air laut.\",\n",
    "\"Penduduk yang dulunya bisa berkehidupan bebas menjadi terpaksa mengungsi ke daratan yang lebih tinggi atau tinggal diatas air.\",\n",
    "\"Apa yang akan menjadi tantangan bagi suatu penduduk ketika terjadi situasi daratan tidak dapat ditinggali kembali karena tengelam oleh naiknya air laut.\"]\n",
    "\n",
    "stimulus_b = [\"Sebuah toko baju berkonsep self-service menawarkan promosi dua buah baju bertema tahun baru seharga Rp50.000,00. sebelum baju bertema tahun baru dibagikan kepada pembeli, sebuah layar akan menampilkan tampilan gambar yang menampilkan kondisi kerja di dalam sebuah pabrik konveksi/pembuatan baju. \",\n",
    "\"Kemudian pembeli diberi program pilihan untuk menyelesaikan pembeliannya atau menyumpangkan Rp50.000,00 untuk dijadikan donasi pembagian baju musim dingin di suatu daerah yang membutuhkan.\",\n",
    "\"Delapan dari sepuluh pembeli memilih untuk memberikan donasi.\",\n",
    "\"Menurut anda mengapa banyak dari pembeli yang memilih berdonasi?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e8a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_a_text = \" \".join(stimulus_a)\n",
    "stimulus_b_text = \" \".join(stimulus_b)\n",
    "\n",
    "for df in [train_a, dev_a, test_a]:\n",
    "    df[\"TEXT\"] = stimulus_a_text + \" [SEP] \" + df[\"RESPONSE\"]\n",
    "\n",
    "for df in [train_b, dev_b, test_b]:\n",
    "    df[\"TEXT\"] = stimulus_b_text + \" [SEP] \" + df[\"RESPONSE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e21a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_a.info())\n",
    "print(train_a.describe(include='all'))  \n",
    "print(train_b.info())\n",
    "print(train_b.describe(include='all'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb777f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='LABEL', data=train_a)\n",
    "plt.title('Distribusi Label (Data_A Train)')\n",
    "plt.show()\n",
    "\n",
    "sns.countplot(x='LABEL', data=train_b)\n",
    "plt.title('Distribusi Label (Data_B Train)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b631e841",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f652ebe",
   "metadata": {},
   "source": [
    "# Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ef6e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_ukara = {'yang', 'lebih', 'untuk', 'akan', 'mereka', 'dan'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab4e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stopwords_ukara]\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64622e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a[\"clean_text\"] = train_a[\"TEXT\"].apply(preprocess)\n",
    "train_b[\"clean_text\"] = train_b[\"TEXT\"].apply(preprocess)\n",
    "test_a[\"clean_text\"] = test_a[\"TEXT\"].apply(preprocess)\n",
    "test_b[\"clean_text\"] = test_b[\"TEXT\"].apply(preprocess)\n",
    "dev_a[\"clean_text\"] = dev_a[\"TEXT\"].apply(preprocess)\n",
    "dev_b[\"clean_text\"] = dev_b[\"TEXT\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74080df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559799d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(texts, labels, tokenizer, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return input_ids, attention_masks, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1c1b62",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147ad151",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ad70c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9daf0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndoBERTClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_labels=2):\n",
    "        super(IndoBERTClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "        self.dropout = torch.nn.Dropout(0.1)\n",
    "        self.classifier = torch.nn.Linear(768, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, epochs=4):\n",
    "    best_val_f1 = 0\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            \n",
    "            model.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # Validation\n",
    "        val_f1, val_accuracy = evaluate_model(model, val_dataloader, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Validation F1: {val_f1:.4f}, Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fb76ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "        \n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        \n",
    "        predictions.extend(np.argmax(logits, axis=1).flatten())\n",
    "        true_labels.extend(label_ids.flatten())\n",
    "    \n",
    "    f1 = f1_score(true_labels, predictions)\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    return f1, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a423eb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(train_texts, train_labels, val_texts, val_labels, test_texts, test_labels, params):\n",
    "    # Encode the data\n",
    "    train_input_ids, train_attention_masks, train_labels = encode_data(train_texts, train_labels, tokenizer)\n",
    "    val_input_ids, val_attention_masks, val_labels = encode_data(val_texts, val_labels, tokenizer)\n",
    "    test_input_ids, test_attention_masks, test_labels = encode_data(test_texts, test_labels, tokenizer)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_data = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=params['batch_size'])\n",
    "    \n",
    "    val_data = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=params['batch_size'])\n",
    "    \n",
    "    test_data = TensorDataset(test_input_ids, test_attention_masks, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=params['batch_size'])\n",
    "    \n",
    "    # Initialize model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = IndoBERTClassifier(num_labels=2)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = AdamW(model.parameters(), lr=params['learning_rate'], eps=1e-8)\n",
    "    total_steps = len(train_dataloader) * params['epochs']\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                              num_warmup_steps=0,\n",
    "                                              num_training_steps=total_steps)\n",
    "    \n",
    "    # Train the model\n",
    "    model = train_model(model, train_dataloader, val_dataloader, optimizer, scheduler, device, params['epochs'])\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_f1, test_accuracy = evaluate_model(model, test_dataloader, device)\n",
    "    \n",
    "    print(f\"\\nTest Results - Accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'test_accuracy': test_accuracy,\n",
    "        'test_f1': test_f1\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e003a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial, train_texts, train_labels, val_texts, val_labels):\n",
    "    params = {\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-5, 5e-5, log=True),\n",
    "        'epochs': trial.suggest_int('epochs', 3, 5)\n",
    "    }\n",
    "    \n",
    "    # Run training with current hyperparameters\n",
    "    results = run_experiment(\n",
    "        train_texts, train_labels, \n",
    "        val_texts, val_labels,\n",
    "        val_texts, val_labels,  # Using val as test for optimization\n",
    "        params\n",
    "    )\n",
    "    \n",
    "    return results['test_f1']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690e14c0",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc15353f",
   "metadata": {},
   "source": [
    "## Data A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6333397",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_a = train_a['clean_text'].tolist()\n",
    "train_labels_a = train_a['LABEL'].values\n",
    "val_texts_a = dev_a['clean_text'].tolist()\n",
    "val_labels_a = dev_a['LABEL'].values\n",
    "test_texts_a = test_a['clean_text'].tolist()\n",
    "test_labels_a = test_a['LABEL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "study_a = optuna.create_study(direction='maximize')\n",
    "study_a.optimize(lambda trial: objective(trial, train_texts_a, train_labels_a, val_texts_a, val_labels_a), n_trials=10)\n",
    "best_params_a = study_a.best_params\n",
    "print(\"Best hyperparameters for Dataset A:\", best_params_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623a1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_a = run_experiment(\n",
    "    train_texts_a, train_labels_a,\n",
    "    val_texts_a, val_labels_a,\n",
    "    test_texts_a, test_labels_a,\n",
    "    best_params_a\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f61dd8a",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff26c62",
   "metadata": {},
   "source": [
    "## Data B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5456c4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts_b = train_b['clean_text'].tolist()\n",
    "train_labels_b = train_b['LABEL'].values\n",
    "val_texts_b = dev_b['clean_text'].tolist()\n",
    "val_labels_b = dev_b['LABEL'].values\n",
    "test_texts_b = test_b['clean_text'].tolist()\n",
    "test_labels_b = test_b['LABEL'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d3515e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "study_b = optuna.create_study(direction='maximize')\n",
    "study_b.optimize(lambda trial: objective(trial, train_texts_b, train_labels_b, val_texts_b, val_labels_b), n_trials=10)\n",
    "best_params_b = study_b.best_params\n",
    "print(\"Best hyperparameters for Dataset B:\", best_params_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b176ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_b = run_experiment(\n",
    "    train_texts_b, train_labels_b,\n",
    "    val_texts_b, val_labels_b,\n",
    "    test_texts_b, test_labels_b,\n",
    "    best_params_b\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabb9686",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Dataset A - Test Accuracy: {final_results_a['test_accuracy']:.4f}, F1: {final_results_a['test_f1']:.4f}\")\n",
    "print(f\"Dataset B - Test Accuracy: {final_results_b['test_accuracy']:.4f}, F1: {final_results_b['test_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29993b6e",
   "metadata": {},
   "source": [
    "# Opsi\n",
    "\n",
    "1. Bert prepocessing + Model\n",
    "2. Bert Prepocessing, Model Neural\n",
    "3. Bert ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b353352",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
